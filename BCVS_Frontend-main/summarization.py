# -*- coding: utf-8 -*-
"""Google Sheets Dynamic Data Extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rUKueotmSrLBkVz-MudNEaWxj34kq4Vl
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install transformers datasets sentencepiece tweet-preprocessor gspread

from flask import request
import json
import pprint
from oauth2client.service_account import ServiceAccountCredentials
import gspread
import re
import os
import torch
import numpy as np
import pandas as pd
from tqdm.auto import tqdm
import preprocessor as p # tweet-preprocessor
import transformers
from collections import defaultdict
from transformers import pipeline, RobertaTokenizerFast, EncoderDecoderModel, BartTokenizer, BartForConditionalGeneration, AutoTokenizer, AutoModelForSeq2SeqLM
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# email = "googlesheetsserviceaccount@my-project-5149-360013.iam.gserviceaccount.com"
scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive',
         'https://www.googleapis.com/auth/drive.file', 'https://www.googleapis.com/auth/spreadsheets']

creds = ServiceAccountCredentials.from_json_keyfile_name('./my-project-5149-google-sheets-service-account-keys.json', scope)
client = gspread.authorize(creds)

# def get_gsheets_data(url, sheet_type):
#   sheet = client.open_by_url(url).sheet1 
#   data = sheet.get_all_values() # get all values from the gsheets
#   df = pd.DataFrame.from_records(data) # convert gspread extracted data from specified gsheets to pd.DataFrame
#   header = df.iloc[0] # consider first row as header 
#   df = df[1:]
#   df.columns = header # assign header 
#   # drop duplicated entries from the supermetrics extracted ghseets data
#   df.drop_duplicates(subset = ["Tweet", "Retweets"], inplace=True)
#   french_tweets = df['Tweet'].tolist() # get french tweets
#   english_tweets = df['Tweet (translated)'].tolist() # get french tweets english translations
#   return french_tweets, english_tweets

# utility methods
# remove punctuations
def remove_punkts(text: str):
  pattern = "[!\"#$%&'()*+,\-/:;<=>?@[\]^_`{|}~.]"
  return re.sub(pattern, '', text)

# preprocess tweets
def preprocess_tweets(tweets: list):
  cleaned_tweets = [remove_punkts(p.clean(tweet)).strip() for tweet in tweets]
  return cleaned_tweets

# genrate tweets passage
def generate_tweets_passage(tweets: list):
  # preprocess tweets!
  cleaned_tweets = preprocess_tweets(tweets)
  cleaned_tweets = list(filter(None, cleaned_tweets))
  passage_tweets = ".".join(cleaned_tweets)  
  return passage_tweets

# save text to .txt file 
def save_2_text(path_to_file: str, text: str):
  with open(path_to_file, 'w') as f:
    f.write(text)

def summarize_english_text(
    text: str,
    model,
    tokenizer,
    max_input_length: int = 512,
    min_output_length: int = 20,
    max_output_length: int = 50,
):
  inputs = tokenizer([text], padding="max_length", truncation=True, max_length=max_input_length, return_tensors="pt")
  summary_ids = model.generate(**inputs.to(device), num_beams=2, min_length=min_output_length, max_length=max_output_length)
  return tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]


def summarize_french_text(
    text: str,
    model,
    tokenizer,
    max_input_length: int = 512,
    min_output_length: int = 200,
    max_output_length: int = 300,
):
  inputs = tokenizer(text, return_tensors="pt", padding="max_length", truncation=True, max_length=max_input_length)
  summary_ids = model.generate(**inputs.to(device), min_length=min_output_length, max_length=max_output_length, no_repeat_ngram_size=2, num_beams=2)
  return tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]

# need to prepare and save a map from keyword -> multiple gsheets urls

def get_gsheets_data(url: str):
  names = [
      'Bank products and Filters Switzerald',
      'Investment Switzeland',
      'Valais',
      'Hypothèques',
      'Fin CH',
      'Crédits CH',
      'Competitors Keyword',
  ]
  spread = client.open_by_url(url) 
  output = defaultdict(list)
  for name, sheet in zip(names, spread.worksheets()[:-1]):
    data = sheet.get_all_values()
    df = pd.DataFrame.from_records(data) # convert gspread extracted data from specified gsheets to pd.DataFrame
    header = df.iloc[0] # consider first row as header 
    df = df[1:]
    df.columns = header # assign header 
    # drop duplicated entries from the supermetrics extracted ghseets data
    df.drop_duplicates(subset = ["Tweet", "Retweets"], inplace=True)
    french_tweets = df['Tweet'].tolist() # get french tweets
    english_tweets = df['Tweet (translated)'].tolist() # get french tweets english translations
    output['keywords'].append(name)
    output['french_tweets'].append(french_tweets)
    output['english_tweets'].append(english_tweets)
    # output['passage_french_tweets'].append(generate_tweets_passage(french_tweets))
    # output['passage_english_tweets'].append(generate_tweets_passage(english_tweets))
  return output
  
class Tweets_Summarizer():
  def __init__(self, gsheets_url, french_tokenizer, french_model, english_tokenizer = None, english_model = None, english=False):
    self.gsheets_url = gsheets_url
    self.french_tokenizer = french_tokenizer
    self.french_model = french_model
    self.english_tokenizer = english_tokenizer
    self.english_model = english_model
    self.english = english
    self.get_french_and_engish_tweets()
    self.process_french_and_english_tweets()

    # save paths
    self.path_to_output_folder = os.path.join(os.getcwd(), "output")
    self.path_to_french_summary_folder = os.path.join(self.path_to_output_folder, "french_summaries")
    self.path_to_english_summary_folder = os.path.join(self.path_to_output_folder, "english_summaries")

  def __repr__(self):
    return f"url: {self.gsheets_url}"

  def __str__(self):
    return str(self.__repr__())

  # def get_french_and_engish_tweets(self):
  #   self.french_tweets, self.english_tweets = get_gsheets_data(self.gsheets_url)

  # def process_french_and_english_tweets(self):
  #   self.passage_french_tweets = generate_tweets_passage(self.french_tweets)
  #   self.passage_english_tweets = generate_tweets_passage(self.english_tweets)

  def get_french_and_engish_tweets(self):
    self.output = get_gsheets_data(self.gsheets_url)

  def process_french_and_english_tweets(self):
    # generate passages for extracted french & english tweets
    for french_tweets_agg in self.output['french_tweets']:
      self.output['passage_french_tweets'].append(generate_tweets_passage(french_tweets_agg))
    if self.english:
      for english_tweets_agg in self.output['english_tweets']:
        self.output['passage_english_tweets'].append(generate_tweets_passage(english_tweets_agg))

  def generate_summaries(self):
    print("generating french summaries")
    french_summary = summarize_french_text(
        text=self.passage_french_tweets,
        model=self.french_model,
        tokenizer=self.french_tokenizer,
        max_input_length=512,
        max_output_length=300,
        min_output_length=200
    )

    if self.english:
      print("generating english summaries")
      english_summary = summarize_english_text(
          text=self.passage_english_tweets,
          model=self.english_model,
          tokenizer=self.english_tokenizer,
          max_input_length=512,
          max_output_length=300,
          min_output_length=200
      )

  def generate_batch_summaries(self):
    print("generating french summaries")
    for passage_french_tweets_agg in tqdm(self.output['passage_french_tweets'], total=len(self.output['passage_french_tweets'])):
      french_summary = summarize_french_text(
          text=passage_french_tweets_agg,
          model=self.french_model,
          tokenizer=self.french_tokenizer,
          max_input_length=512,
          max_output_length=300,
          min_output_length=100
      )
      self.output['french_summary'].append(french_summary)

    if self.english:
      print("generating english summaries")
      for passage_english_tweets_agg in tqdm(self.output['passage_english_tweets'], total=len(self.output['passage_english_tweets'])):
        english_summary = summarize_english_text(
            text=passage_english_tweets_agg,
            model=self.english_model,
            tokenizer=self.english_tokenizer,
            max_input_length=512,
            max_output_length=300,
            min_output_length=200
        )

    # print(f"saving files locally under {self.path_to_output_folder}")
    # french_summary_filename = f"{self.process_code}.txt"
    # english_summary_filename = f"{self.process_code}.txt"
    # print(self.path_to_french_summary_folder)
    # save_2_text(path_to_file = os.path.join(self.path_to_french_summary_folder, french_summary_filename), text = french_summary)
    # print(self.path_to_english_summary_folder)
    # save_2_text(path_to_file = os.path.join(self.path_to_english_summary_folder, english_summary_filename), text = english_summary)
    return self.output

# if __name__ == "__main__":
#   # define model checkpoints
#   model_checkpoints_english = "facebook/bart-large-cnn"
#   model_checkpoints_french = "csebuetnlp/mT5_multilingual_XLSum"
#   # check for cuda availability
#   

#   # initialize model and tokenizer
#   french_tokenizer = AutoTokenizer.from_pretrained(model_checkpoints_french)
#   french_model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints_french).to(device)

#   # english_tokenizer = BartTokenizer.from_pretrained(model_checkpoints_english)
#   # english_model = BartForConditionalGeneration.from_pretrained(model_checkpoints_english).to(device)

#   # define Tweets_Summarizer object
#   tweets_summarizer = Tweets_Summarizer(
#     gsheets_url = "https://docs.google.com/spreadsheets/d/1Q8Cj8zx5-iOqrnaRB2H1dZCHYcXjkjdGXwGjBhpBPMU/edit",
#     french_tokenizer = french_tokenizer,
#     french_model = french_model
#   )
  
#   # generate batch summaries
#   output = tweets_summarizer.generate_batch_summaries()

